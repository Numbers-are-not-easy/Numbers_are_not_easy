{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94462a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"hugging_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "DATASET_JSONL = \"{Dataset_path}/sampled_arithmetic.jsonl\"\n",
    "\n",
    "\n",
    "SELECTED_MODEL = \"gemma\"  # options: qwen | gemma | mistral\n",
    "\n",
    "# Resume options\n",
    "START_INDEX = 0     # change if resuming (e.g. 6568)\n",
    "END_INDEX   = None   # None = go to end \n",
    "\n",
    "# Map\n",
    "MODEL_MAP = {\n",
    "    \"qwen\":    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"gemma\":   \"google/gemma-2-9b-it\",\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "}\n",
    "\n",
    "\n",
    "MODEL_NAME   = MODEL_MAP[SELECTED_MODEL]\n",
    "OUTPUT_DIR   = f\"/kaggle/working/{SELECTED_MODEL}_outputs\"\n",
    "OUTPUT_JSONL = f\"{SELECTED_MODEL}_predictions.jsonl\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "BATCH_SIZE     = 3\n",
    "MAX_NEW_TOKENS = 256\n",
    "TEMPERATURE    = 0.0\n",
    "TOP_P          = 0.9\n",
    "USE_8BIT       = False        \n",
    "USE_BF16       = False\n",
    "SEED           = 13\n",
    "\n",
    "# Time budget: 11h55m (with 1min safety)\n",
    "MAX_SECONDS       = 11*3600 + 55*60\n",
    "SAFETY_BUFFER_SEC = 60\n",
    "SAVE_EVERY        = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Print Logs\n",
    "# =========================\n",
    "print(f\"OUTPUT_DIR:{OUTPUT_DIR}\")\n",
    "print(f\"MODEL_NAME:{MODEL_NAME}\")\n",
    "print(f\"START_INDEX:{START_INDEX }\")\n",
    "print(f\"BATCH_SIZE :{BATCH_SIZE }\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Hugging Face login\n",
    "# =========================\n",
    "login(hf_token)\n",
    "\n",
    "# =========================\n",
    "# Reproducibility\n",
    "# =========================\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# =========================\n",
    "# Load model & tokenizer\n",
    "# =========================\n",
    "kwargs = {}\n",
    "if USE_8BIT:\n",
    "    kwargs.update(dict(load_in_8bit=True, device_map=\"auto\"))\n",
    "else:\n",
    "    torch_dtype = torch.bfloat16 if USE_BF16 and torch.cuda.is_available() else torch.float16\n",
    "    kwargs.update(dict(torch_dtype=torch_dtype, device_map=\"auto\"))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **kwargs)\n",
    "model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True  # extra speed\n",
    "\n",
    "# =========================\n",
    "# IO helpers\n",
    "# =========================\n",
    "out_dir = Path(OUTPUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / OUTPUT_JSONL\n",
    "\n",
    "def stream_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "def load_completed_ids(existing_path: Path) -> Dict[str,int]:\n",
    "    done = {}\n",
    "    if existing_path.exists():\n",
    "        with open(existing_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                    if obj.get(\"id\"): done[obj[\"id\"]] = 1\n",
    "                except: continue\n",
    "    return done\n",
    "\n",
    "def append_jsonl(records: List[Dict[str, Any]], path: Path) -> None:\n",
    "    if not records: return\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for r in records:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Batched generation\n",
    "# =========================\n",
    "@torch.inference_mode()\n",
    "def generate_batch(prompts: List[str]) -> List[str]:\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True,\n",
    "                       truncation=True, max_length=4096).to(model.device)\n",
    "\n",
    "    gen_cfg = GenerationConfig(\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        do_sample=TEMPERATURE > 0.0,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    outputs = model.generate(**inputs, generation_config=gen_cfg, use_cache=True)\n",
    "\n",
    "    decoded = []\n",
    "    for i in range(len(prompts)):\n",
    "        prompt_len = inputs[\"input_ids\"][i].shape[0]\n",
    "        out_ids = outputs[i][prompt_len:]\n",
    "        text = tokenizer.decode(out_ids, skip_special_tokens=True).strip()\n",
    "        if \"Assistant:\" in text: text = text.split(\"Assistant:\")[-1].strip()\n",
    "        decoded.append(text)\n",
    "    return decoded\n",
    "\n",
    "# =========================\n",
    "# Time zone handling\n",
    "# ========================   \n",
    "def get_timezone_context(obj):\n",
    "    if obj.get(\"tz_src\") or obj.get(\"tz_dst\"):\n",
    "        return {\n",
    "            \"tz_src\": obj.get(\"tz_src\"),\n",
    "            \"tz_dst\": obj.get(\"tz_dst\"),\n",
    "        }\n",
    "    if obj.get(\"tz\"):\n",
    "        return {\"tz\": obj[\"tz\"]}\n",
    "    return {}\n",
    "# =========================\n",
    "# Build Prompt\n",
    "# ========================    \n",
    "def build_prompt(payload: dict) -> str:\n",
    "    lines = [\n",
    "        \"You are a smart arithmetic solver. Compute exactly.\",\n",
    "        f\"Task ID: {payload['id']}\",\n",
    "        f\"Task type: {payload['type']}\",\n",
    "    ]\n",
    "    tz_info = payload[\"tz_info\"]\n",
    "    if tz_info.get(\"tz_src\"):\n",
    "        lines.append(f\"Source timezone: {tz_info['tz_src']}\")\n",
    "    if tz_info.get(\"tz_dst\"):\n",
    "        lines.append(f\"Destination timezone: {tz_info['tz_dst']}\")\n",
    "    if tz_info.get(\"tz\"):\n",
    "        lines.append(f\"Timezone: {tz_info['tz']}\")\n",
    "    lines.append(f\"Task: {payload['task_text']}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# =========================\n",
    "# Main loop with monitoring\n",
    "# =========================\n",
    "def run_full_pass(dataset_path, output_path, start_index=0, end_index=None,\n",
    "                  batch_size=BATCH_SIZE, save_every=SAVE_EVERY,\n",
    "                  max_seconds=MAX_SECONDS, safety_buffer_sec=SAFETY_BUFFER_SEC):\n",
    "\n",
    "    start = time.time(); deadline = start + max_seconds - safety_buffer_sec\n",
    "    data = stream_jsonl(dataset_path)\n",
    "    if end_index is None: end_index = len(data)\n",
    "    data = data[start_index:end_index]\n",
    "\n",
    "    completed = load_completed_ids(output_path)\n",
    "    processed = len(completed); written_since_last = 0\n",
    "    buffer, prompts, metas = [], [], []\n",
    "\n",
    "\n",
    "    for idx, obj in enumerate(data, start=start_index + 1):\n",
    "        _id = obj.get(\"id\", f\"item_{idx}\")\n",
    "        if _id in completed:\n",
    "            continue\n",
    "\n",
    "        prompt_payload = {\n",
    "            \"id\": _id,\n",
    "            \"type\": obj.get(\"type\"),\n",
    "            \"task_text\": (\n",
    "                obj.get(\"task_text\")\n",
    "                or obj.get(\"input_text\")\n",
    "                or obj.get(\"instruction\")\n",
    "                or \"\"\n",
    "            ),\n",
    "            \"tz_info\": get_timezone_context(obj),\n",
    "        }\n",
    "\n",
    "        prompt_text = build_prompt(prompt_payload)        # string\n",
    "        prompts.append(prompt_text)                       # list of strings\n",
    "        metas.append(prompt_payload)   \n",
    "\n",
    "        if len(prompts) >= batch_size:\n",
    "            if time.time() >= deadline:\n",
    "                print(f\"[STOP] Time budget hit, processed={processed}, seen={idx}\")\n",
    "                break\n",
    "\n",
    "            outs = generate_batch(prompts)\n",
    "            for meta, text in zip(metas, outs):\n",
    "                buffer.append({\"id\": meta[\"id\"], \"type\": meta.get(\"type\"), \"raw_response\": text})\n",
    "                processed += 1; written_since_last += 1\n",
    "\n",
    "            if written_since_last >= save_every:\n",
    "                append_jsonl(buffer, output_path); buffer.clear(); written_since_last = 0\n",
    "\n",
    "            elapsed = time.time()-start; remaining = max(0, deadline-time.time())\n",
    "            print(f\"[PROGRESS] answered={processed} | seen={idx} | \"\n",
    "                  f\"elapsed={int(elapsed//3600)}h{int((elapsed%3600)//60)}m | \"\n",
    "                  f\"remaining≈{int(remaining//3600)}h{int((remaining%3600)//60)}m\")\n",
    "\n",
    "            prompts.clear(); metas.clear()\n",
    "\n",
    "    # flush leftovers\n",
    "    if prompts and time.time() < deadline:\n",
    "        outs = generate_batch(prompts)\n",
    "        for meta, text in zip(metas, outs):\n",
    "            buffer.append({\"id\": meta[\"id\"], \"type\": meta.get(\"type\"), \"raw_response\": text})\n",
    "            processed += 1\n",
    "\n",
    "    if buffer:\n",
    "        append_jsonl(buffer, output_path)\n",
    "        print(f\"[FLUSH] wrote {len(buffer)} records\")\n",
    "\n",
    "    elapsed = time.time()-start\n",
    "    print(f\"[DONE] answered={processed} | elapsed={int(elapsed//3600)}h{int((elapsed%3600)//60)}m\")\n",
    "\n",
    "# =========================\n",
    "# Run full pass\n",
    "# =========================\n",
    "run_full_pass(\n",
    "    dataset_path=DATASET_JSONL,\n",
    "    output_path=out_path,\n",
    "    start_index=START_INDEX,\n",
    "    end_index=END_INDEX,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
